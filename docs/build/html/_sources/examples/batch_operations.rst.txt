Batch Operations Examples
=========================

This guide demonstrates Groggy's high-performance batch operations for efficient large-scale graph processing.

Overview
--------

Groggy's batch operations provide significant performance improvements over individual operations:

- **10-100x faster** for bulk operations
- **Memory efficient** bulk processing
- **Identical API** across Python and Rust backends
- **Linear scaling** to millions of nodes

Social Network Analysis
-----------------------

Building Large Social Networks Efficiently
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   import groggy
   import random
   import time
   
   # Use Rust backend for maximum performance
   gli.set_backend('rust')
   g = gli.Graph()
   
   print("Creating social network...")
   start_time = time.time()
   
   # Sample demographic data
   cities = ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix']
   occupations = ['Engineer', 'Teacher', 'Doctor', 'Artist', 'Manager']
   first_names = ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank']
   
   # Create 10,000 people efficiently using batch operations
   people_data = []
   for i in range(10000):
       people_data.append({
           'id': f"person_{i}",
           'name': f"{random.choice(first_names)}_{i}",
           'age': random.randint(18, 65),
           'city': random.choice(cities),
           'occupation': random.choice(occupations),
           'active': random.choice([True, False])
       })
   
   # Batch add all people at once - much faster than individual adds
   g.add_nodes(people_data)
   
   # Create friendship connections
   friendship_data = []
   node_ids = [person['id'] for person in people_data]
   
   for _ in range(5000):
       person1 = random.choice(node_ids)
       person2 = random.choice(node_ids)
       if person1 != person2:
           friendship_data.append({
               'source': person1,
               'target': person2,
               'relationship': 'friend',
               'strength': random.uniform(0.3, 1.0)
           })
   
   # Batch add all friendships at once
   g.add_edges(friendship_data)
   
   creation_time = time.time() - start_time
   print(f"Created {g.node_count():,} people and {g.edge_count():,} friendships in {creation_time:.2f} seconds")

Efficient Filtering and Analysis
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   # Efficient filtering using the new API
   print("\\nPerforming batch analysis...")
   
   # Find people by city using dictionary filters
   ny_people = g.filter_nodes({'city': 'New York'})
   la_people = g.filter_nodes({'city': 'Los Angeles'})
   
   # Find people by occupation
   engineers = g.filter_nodes({'occupation': 'Engineer'})
   teachers = g.filter_nodes({'occupation': 'Teacher'})
   
   # Find active users
   active_users = g.filter_nodes({'active': True})
   
   # Complex lambda-based filtering
   young_professionals = g.filter_nodes(
       lambda node_id, attrs: attrs.get('age', 0) < 30 and attrs.get('occupation') in ['Engineer', 'Doctor']
   )
   
   print(f"New York residents: {len(ny_people):,}")
   print(f"Los Angeles residents: {len(la_people):,}")
   print(f"Engineers: {len(engineers):,}")
   print(f"Teachers: {len(teachers):,}")
   print(f"Active users: {len(active_users):,}")
   print(f"Young professionals: {len(young_professionals):,}")

Bulk Attribute Updates
~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   # Bulk updates for performance - promote some engineers
   promotion_updates = {}
   senior_engineers = g.filter_nodes(
       lambda node_id, attrs: (attrs.get('occupation') == 'Engineer' and 
                              attrs.get('age', 0) > 35)
   )
   
   for engineer_id in senior_engineers[:50]:  # Promote first 50
       promotion_updates[engineer_id] = {
           'title': 'Senior Engineer',
           'salary': 120000,
           'promotion_date': '2025-01-01'
       }
   
   # Apply all updates efficiently
   g.update_nodes(promotion_updates)
   print(f"Promoted {len(promotion_updates)} engineers to senior level")
Using Context Manager for Maximum Performance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   # Use context manager for optimal batch performance
   print("\\nUsing batch context for maximum efficiency...")
   
   with g.batch_operations() as batch:
       # Add many nodes within context
       for i in range(1000):
           batch.add_node(f"batch_person_{i}", 
                         name=f"BatchPerson_{i}",
                         category="batch_added")
       
       # Add edges within context
       for i in range(500):
           source = f"batch_person_{i}"
           target = f"batch_person_{i+1}"
           batch.add_edge(source, target, connection_type="sequential")
       
       # Bulk attribute updates within context
       updates = {f"batch_person_{i}": {"processed": True} for i in range(100)}
       batch.set_node_attributes(updates)
   
   print(f"Final graph size: {g.node_count():,} nodes, {g.edge_count():,} edges")

Performance Comparison Example
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   import time
   
   # Compare individual vs batch operations
   def time_individual_operations(graph, num_operations=1000):
       start = time.time()
       for i in range(num_operations):
           graph.add_node(f"individual_{i}", value=i)
           if i > 0:
               graph.add_edge(f"individual_{i-1}", f"individual_{i}")
       return time.time() - start
   
   def time_batch_operations(graph, num_operations=1000):
       start = time.time()
       
       # Prepare data
       nodes_data = [{'id': f"batch_{i}", 'value': i} for i in range(num_operations)]
       edges_data = [{'source': f"batch_{i}", 'target': f"batch_{i+1}"} 
                     for i in range(num_operations-1)]
       
       # Batch operations
       graph.add_nodes(nodes_data)
       graph.add_edges(edges_data)
       
       return time.time() - start
   
   # Create separate graphs for fair comparison
   g1 = gli.Graph()
   g2 = gli.Graph()
   
   individual_time = time_individual_operations(g1, 1000)
   batch_time = time_batch_operations(g2, 1000)
   
   print(f"Individual operations: {individual_time:.4f}s")
   print(f"Batch operations: {batch_time:.4f}s")
   print(f"Speedup: {individual_time/batch_time:.1f}x")

Supply Chain Network Analysis
-----------------------------

Enterprise-Scale Graph Processing
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   # Create supply chain network using new batch API
   supply_chain = gli.Graph(backend='rust')
   
   # Company types and attributes
   company_types = ['Supplier', 'Manufacturer', 'Distributor', 'Retailer']
   industries = ['Electronics', 'Automotive', 'Textiles', 'Food', 'Pharma']
   regions = ['North America', 'Europe', 'Asia', 'South America']
   
   # Create companies using batch operations
   companies_data = []
   for i in range(5000):
       companies_data.append({
           'id': f"company_{i}",
           'name': f"Company_{i}",
           'type': random.choice(company_types),
           'industry': random.choice(industries),
           'region': random.choice(regions),
           'size': random.choice(['Small', 'Medium', 'Large']),
           'sustainability_score': random.uniform(1.0, 10.0),
           'financial_rating': random.choice(['A', 'B', 'C', 'D'])
       })
   
   supply_chain.add_nodes(companies_data)
   
   # Create supply relationships using batch operations
   relationships_data = []
   company_ids = [c['id'] for c in companies_data]
   
   for _ in range(8000):
       supplier = random.choice(company_ids)
       customer = random.choice(company_ids)
       if supplier != customer:
           relationships_data.append({
               'source': supplier,
               'target': customer,
               'relationship': 'supplies',
               'volume': random.randint(1000, 100000),
               'contract_value': random.randint(10000, 5000000),
               'risk_level': random.choice(['Low', 'Medium', 'High'])
           })
   
   supply_chain.add_edges(relationships_data)

Supply Chain Risk Analysis
~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   # Risk analysis using the new filtering API
   print("\\nSupply Chain Risk Analysis:")
   
   # Find high-risk supply relationships
   high_risk_edges = supply_chain.filter_edges({'risk_level': 'High'})
   print(f"High-risk supply relationships: {len(high_risk_edges)}")
   
   # Find companies by sustainability score
   sustainable_companies = supply_chain.filter_nodes(
       lambda node_id, attrs: attrs.get('sustainability_score', 0) >= 8.0
   )
   print(f"Highly sustainable companies: {len(sustainable_companies)}")
   
   # Regional analysis
   for region in regions:
       regional_companies = supply_chain.filter_nodes({'region': region})
       print(f"{region}: {len(regional_companies)} companies")
       
       # Calculate regional statistics
       region_scores = []
       large_count = 0
       for company_id in regional_companies:
           company = supply_chain.get_node(company_id)
           region_scores.append(company.attributes['sustainability_score'])
           if company.attributes['size'] == 'Large':
               large_count += 1
       
       if region_scores:
           avg_sustainability = sum(region_scores) / len(region_scores)
           print(f"  Average sustainability: {avg_sustainability:.2f}")
           print(f"  Large companies: {large_count}")

Bulk Attribute Updates
~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   # Quarterly updates - efficient bulk operations
   quarterly_updates = {}
   
   # Update all companies in electronics industry
   electronics_companies = supply_chain.filter_nodes({'industry': 'Electronics'})
   for company_id in electronics_companies:
       quarterly_updates[company_id] = {
           'last_audit': '2024-Q1',
           'market_trend': 'growing'
       }
   
   # Apply all updates at once
   supply_chain.update_nodes(quarterly_updates)
   
   # Update large companies with growth metrics
   large_company_updates = {}
   large_companies = supply_chain.filter_nodes({'size': 'Large'})
   
   for company_id in large_companies[:100]:  # Update first 100
       current_node = supply_chain.get_node(company_id)
       large_company_updates[company_id] = {
           'market_cap': random.randint(1000000, 10000000),
           'employees': random.randint(1000, 50000),
           'updated': True
       }
   
   supply_chain.update_nodes(large_company_updates)

Knowledge Graph Processing
-------------------------

Academic Collaboration Network
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   # Create academic collaboration network with batch operations
   academic_graph = gli.Graph(backend='rust')
   
   # Academic data
   fields = ['Computer Science', 'Biology', 'Physics', 'Chemistry', 'Mathematics']
   institutions = ['MIT', 'Stanford', 'Harvard', 'Caltech', 'Berkeley']
   positions = ['Professor', 'Associate Professor', 'Assistant Professor', 'Postdoc']
   
   # Create researchers using batch operations
   researchers_data = []
   for i in range(3000):
       researchers_data.append({
           'id': f"researcher_{i}",
           'name': f"Dr. {random.choice(['Smith', 'Johnson', 'Williams', 'Brown'])}_{i}",
           'field': random.choice(fields),
           'institution': random.choice(institutions),
           'position': random.choice(positions),
           'h_index': random.randint(5, 100),
           'publications': random.randint(10, 200),
           'years_active': random.randint(5, 30)
       })
   
   academic_graph.add_nodes(researchers_data)
   
   # Create collaborations using batch operations
   collaborations_data = []
   researcher_ids = [r['id'] for r in researchers_data]
   
   for _ in range(5000):
       researcher1 = random.choice(researcher_ids)
       researcher2 = random.choice(researcher_ids)
       if researcher1 != researcher2:
           collaborations_data.append({
               'source': researcher1,
               'target': researcher2,
               'relationship': 'collaboration',
               'papers_coauthored': random.randint(1, 20),
               'collaboration_strength': random.uniform(0.1, 1.0),
               'active': random.choice([True, False])
           })
   
   academic_graph.add_edges(collaborations_data)

Research Impact Analysis
~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   # Analyze research network using new filtering API
   print("\\nAcademic Network Analysis:")
   
   # Find top researchers by field
   for field in fields:
       field_researchers = academic_graph.filter_nodes({'field': field})
       
       # Get top researchers by h-index
       top_researchers = []
       for researcher_id in field_researchers:
           researcher = academic_graph.get_node(researcher_id)
           top_researchers.append((researcher_id, researcher.attributes))
       
       # Sort by h-index
       top_researchers.sort(key=lambda x: x[1]['h_index'], reverse=True)
       
       print(f"\\nTop {field} researchers:")
       for researcher_id, attrs in top_researchers[:5]:
           print(f"  {attrs['name']}: h-index {attrs['h_index']}, "
                 f"{attrs['publications']} publications")
   
   # Cross-institutional collaborations
   active_collabs = academic_graph.filter_edges({'active': True})
   cross_institutional = []
   
   for edge_id in active_collabs:
       edge = academic_graph.edges[edge_id]
       source_attrs = academic_graph.get_node(edge.source).attributes
       target_attrs = academic_graph.get_node(edge.target).attributes
       if source_attrs['institution'] != target_attrs['institution']:
           cross_institutional.append((edge.source, edge.target))
   
   print(f"\\nCross-institutional collaborations: {len(cross_institutional)}")

Performance Monitoring
---------------------

Benchmarking Batch Operations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   import time
   
   def benchmark_operations(graph, description):
       \"\"\"Benchmark various operations\"\"\"
       print(f"\\n{description}")
       print("-" * len(description))
       
       # Benchmark filtering
       start = time.time()
       filtered_nodes = graph.filter_nodes({'active': True})
       filter_time = time.time() - start
       print(f"Filter operation: {len(filtered_nodes)} nodes in {filter_time:.4f}s")
       
       # Benchmark bulk updates
       start = time.time()
       updates = {node_id: {'last_checked': '2025-01-01'} for node_id in filtered_nodes[:1000]}
       graph.update_nodes(updates)
       update_time = time.time() - start
       print(f"Bulk update: 1000 nodes in {update_time:.4f}s")
       
       # Benchmark neighbor queries
       start = time.time()
       sample_nodes = list(graph.get_node_ids())[:100]
       neighbor_counts = []
       for node_id in sample_nodes:
           neighbors = graph.get_neighbors(node_id)
           neighbor_counts.append(len(neighbors))
       neighbor_time = time.time() - start
       avg_neighbors = sum(neighbor_counts) / len(neighbor_counts)
       print(f"Neighbor queries: 100 nodes in {neighbor_time:.4f}s, avg {avg_neighbors:.1f} neighbors")
   
   # Benchmark different graph sizes
   for size in [1000, 5000, 10000]:
       test_graph = gli.Graph(backend='rust')
       
       # Create test data
       nodes_data = [{'id': f'node_{i}', 'active': i % 2 == 0, 'value': i} 
                     for i in range(size)]
       edges_data = [{'source': f'node_{i}', 'target': f'node_{(i+1) % size}', 'weight': 1.0} 
                     for i in range(size//2)]
       
       # Time graph creation
       start = time.time()
       test_graph.add_nodes(nodes_data)
       test_graph.add_edges(edges_data)
       creation_time = time.time() - start
       
       print(f"\\nGraph creation ({size} nodes): {creation_time:.4f}s")
       benchmark_operations(test_graph, f"Operations on {size}-node graph")

Best Practices for Batch Operations
-----------------------------------

1. **Use Batch Methods**: Always prefer `add_nodes()`, `add_edges()`, and `update_nodes()` over individual operations
2. **Context Managers**: Use `batch_operations()` context manager for optimal performance
3. **Backend Selection**: Use Rust backend for large graphs (`backend='rust'`)
4. **Memory Management**: Process large datasets in chunks to avoid memory issues
5. **Filtering Strategy**: Use dictionary filters for simple cases, lambda functions for complex logic

.. code-block:: python

   # Example of optimal batch processing pattern
   def process_large_dataset(data_source, chunk_size=10000):
       graph = gli.Graph(backend='rust')
       
       # Process in chunks
       for chunk_start in range(0, len(data_source), chunk_size):
           chunk_end = min(chunk_start + chunk_size, len(data_source))
           chunk_data = data_source[chunk_start:chunk_end]
           
           # Prepare batch data
           nodes_data = []
           edges_data = []
           
           for item in chunk_data:
               nodes_data.append({
                   'id': item['id'],
                   'attributes': item['attributes']
               })
               
               if 'connections' in item:
                   for connection in item['connections']:
                       edges_data.append({
                           'source': item['id'],
                           'target': connection['target'],
                           'attributes': connection['attributes']
                       })
           
           # Batch process chunk
           with graph.batch_operations() as batch:
               graph.add_nodes(nodes_data)
               graph.add_edges(edges_data)
           
           print(f"Processed chunk {chunk_start//chunk_size + 1}, "
                 f"total nodes: {graph.node_count():,}")
       
       return graph

   # Usage
   # large_graph = process_large_dataset(your_data_source)

This completes the comprehensive batch operations guide with the updated Groggy API.
       updates = {node_id: {'last_accessed': '2024-01-15'} 
                  for node_id in filtered_nodes[:500]}
       start = time.time()
       graph.batch_set_node_attributes(updates)
       update_time = time.time() - start
       print(f"Batch updates: 500 nodes in {update_time:.4f}s")
   
   # Test with different backends
   gli.set_backend('rust')
   rust_graph = gli.Graph()
   # ... populate graph ...
   benchmark_operations(rust_graph, "Rust Backend Performance")
   
   gli.set_backend('python')
   python_graph = gli.Graph()
   # ... populate graph ...
   benchmark_operations(python_graph, "Python Backend Performance")

Best Practices Summary
---------------------

Performance Optimization Guidelines
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

1. **Use Batch Operations**: Always prefer batch methods for multiple operations

   .. code-block:: python
   
      # ✅ Good
      filtered = g.batch_filter_nodes(category='important')
      attributes = g.batch_get_node_attributes(filtered)
      
      # ❌ Avoid
      filtered = [n for n in g.nodes if g.get_node(n)['category'] == 'important']

2. **Choose the Right Backend**: Use Rust for large graphs, Python for development

   .. code-block:: python
   
      # Production/large graphs
      gli.set_backend('rust')
      
      # Development/debugging
      gli.set_backend('python')

3. **Batch Attribute Operations**: Update multiple nodes efficiently

   .. code-block:: python
   
      # ✅ Efficient bulk updates
      g.batch_set_node_attributes({
          'node1': {'status': 'active'},
          'node2': {'status': 'inactive'}
      })

4. **Memory-Efficient Processing**: Use subgraphs for focused analysis

   .. code-block:: python
   
      # Process subsets efficiently
      important_subgraph = g.create_subgraph_fast(priority='high')
      analysis_results = analyze_network(important_subgraph)

Expected Performance Gains
~~~~~~~~~~~~~~~~~~~~~~~~~~

With proper use of batch operations, expect:

- **30-40x faster** filtering operations
- **Linear scaling** to millions of nodes  
- **50-70% less memory** usage with Rust backend
- **Sub-millisecond** query times for common operations

These optimizations make Groggy suitable for real-time applications and large-scale graph analysis.
