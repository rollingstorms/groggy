# Remaining Performance Bottlenecks - Comprehensive Analysis

## Overview

After optimizing Betweenness and Closeness (3.3x speedup), we've established patterns and now need to identify remaining bottlenecks systematically.

---

## üî¥ CRITICAL: Louvain Algorithm (~4100ms)

### Current Performance
- **500 nodes, 2500 edges**: 4135ms (4.1 seconds!)
- **100 nodes, 250 edges**: 40ms
- **Scaling**: O(n¬≥) or worse behavior

### Root Cause Analysis

**Line 147 in `src/algorithms/community/louvain.rs`:**
```rust
for _ in 0..self.max_iter {              // Up to 20 iterations
    for &node in &snapshot.nodes {       // 500 nodes
        for &candidate in &candidate_comms {  // ~10 candidates
            let mut test_partition = partition.clone();  // ‚ùå CLONE ENTIRE HashMap!
            test_partition.insert(node, candidate);
            let q = modularity(&test_partition, &snapshot.edges, &modularity_data);
        }
    }
}
```

**Problem**: Cloning partition HashMap for EVERY candidate move!
- 20 iterations √ó 500 nodes √ó 10 candidates = **100,000 HashMap clones**
- Each clone: 500 entries √ó (hash + copy) = expensive!

### Extensive Fix Plan

#### Option 1: Incremental Modularity (Recommended)
**Concept**: Calculate modularity DELTA instead of full recomputation

```rust
// ‚ùå OLD: Clone and recompute
let mut test_partition = partition.clone();
test_partition.insert(node, candidate);
let q = modularity(&test_partition, &edges, &data);

// ‚úÖ NEW: Calculate delta only
let delta_q = modularity_delta(
    node, 
    current_comm, 
    candidate,
    &partition,
    &adjacency,
    &community_degrees,
    total_weight
);
let new_q = baseline_q + delta_q;
```

**Benefits**:
- No clones needed
- O(degree) instead of O(E) per move
- **Expected: 50-100x speedup** (4s ‚Üí 40-80ms)

**Implementation Steps**:
1. Add `modularity_delta()` function
2. Track community degrees incrementally
3. Update only when move is accepted
4. Estimated effort: 4-6 hours

#### Option 2: Copy-on-Write Partition Structure
**Concept**: Use persistent data structure for cheap "clones"

```rust
use im::HashMap;  // Persistent HashMap with structural sharing

let mut partition = im::HashMap::new();
// Clones are O(1) with structural sharing
let test_partition = partition.update(node, candidate);
```

**Benefits**:
- Simple drop-in replacement
- O(1) "clone" operations
- **Expected: 10-20x speedup**

**Implementation Steps**:
1. Add `im` crate dependency
2. Replace `HashMap` with `im::HashMap` for partition
3. Test modularity calculations
4. Estimated effort: 2-3 hours

#### Option 3: Single-Pass Move Evaluation
**Concept**: Don't clone, just calculate "what if"

```rust
// Track deltas without mutating partition
let gain = calculate_move_gain(node, current_comm, candidate, &state);
if gain > best_gain {
    best_gain = gain;
    best_comm = candidate;
}
// Apply best move ONCE after evaluation
partition.insert(node, best_comm);
```

**Benefits**:
- Zero clones in evaluation phase
- Simpler logic
- **Expected: 20-30x speedup**

**Implementation Steps**:
1. Refactor to separate evaluation from application
2. Calculate gains using local state
3. Apply best move once
4. Estimated effort: 3-4 hours

### Recommended Approach
**Option 1 (Incremental Modularity)** - Industry standard, best performance, more complex but highest reward.

---

## üü° MEDIUM: Framework Overhead (~0.27ms per algorithm)

### Current Impact
- Pipeline building: ~0.05ms
- view() HashSet copy: ~0.05ms
- Pipeline clone: ~0.05ms
- **Total: 35% overhead on fast algorithms**

### Fix Plan: Lightweight Subgraph Views (P3)

#### Current Problem
```rust
// In Graph.view()
let all_nodes: Vec<NodeId> = graph.node_ids();  // O(V)
let all_edges: Vec<EdgeId> = graph.edge_ids();  // O(E)
let mut node_set = HashSet::with_capacity(all_nodes.len());
let mut edge_set = HashSet::with_capacity(all_edges.len());
node_set.extend(all_nodes);  // ‚ùå Copy all node IDs
edge_set.extend(all_edges);  // ‚ùå Copy all edge IDs
```

#### Solution: Tagged Union for Subgraph Mode

```rust
enum SubgraphData {
    FullGraph,  // Just a marker - no sets needed!
    Filtered { 
        nodes: BitSet,  // 500 nodes = 64 bytes vs 4KB
        edges: BitSet,  // 2500 edges = 320 bytes vs 20KB
    }
}

pub struct Subgraph {
    graph: Rc<RefCell<Graph>>,
    data: SubgraphData,
    subgraph_type: String,
}

impl SubgraphOperations for Subgraph {
    fn contains_node(&self, node: NodeId) -> bool {
        match &self.data {
            SubgraphData::FullGraph => true,  // O(1)!
            SubgraphData::Filtered { nodes, .. } => nodes.contains(node),
        }
    }
}
```

**Benefits**:
- Full-graph subgraph: 0 bytes (just marker)
- Filtered subgraph: 10-20x smaller (bitsets vs HashSets)
- **Expected: 0.05-0.1ms saved per algorithm**

**Implementation Steps**:
1. Create SubgraphData enum
2. Update Subgraph struct
3. Modify SubgraphOperations methods to match on enum
4. Update view() to return FullGraph variant
5. Update filtering methods to create Filtered variant
6. Estimated effort: 6-8 hours

---

## üü° MEDIUM: Pipeline Build Overhead (~0.05ms per call, P2)

### Current Problem
```rust
pub fn apply(&self, py: Python, algorithm: &PyAny) -> PyResult<PySubgraph> {
    // Build pipeline from scratch
    let handle = py_build_pipeline(py, spec_list)?;
    let result = py_run_pipeline(py, &handle, self);
    py_drop_pipeline(&handle);  // ‚ùå Throw away immediately
    result
}
```

**Issue**: Every `.apply()` call:
1. Converts Python dict ‚Üí Rust AlgorithmSpec
2. Looks up algorithm in registry
3. Builds Pipeline object
4. Runs it
5. **Drops it immediately!**

### Fix Plan: Algorithm Handle Cache

```rust
use std::sync::Arc;
use lru::LruCache;

static ALGORITHM_CACHE: OnceLock<Mutex<LruCache<String, Arc<Box<dyn Algorithm>>>>> = ...;

pub fn apply(&self, py: Python, algorithm: &PyAny) -> PyResult<PySubgraph> {
    // Get algorithm spec hash
    let spec_hash = hash_algorithm_spec(algorithm)?;
    
    // Try cache
    let mut cache = ALGORITHM_CACHE.get_or_init(...).lock().unwrap();
    let algo = match cache.get(&spec_hash) {
        Some(cached) => cached.clone(),  // Arc clone is cheap
        None => {
            // Build and cache
            let algo = build_algorithm(algorithm)?;
            let algo_arc = Arc::new(algo);
            cache.put(spec_hash.clone(), algo_arc.clone());
            algo_arc
        }
    };
    
    // Execute directly
    let mut ctx = Context::new();
    let result = algo.execute(&mut ctx, self.inner.clone())?;
    PySubgraph::from_core_subgraph(result)
}
```

**Benefits**:
- First call: Same cost (build + cache)
- Subsequent calls: ~0.05ms faster
- **Expected: 20% faster for repeated algorithms**

**Implementation Steps**:
1. Add LRU cache for algorithms
2. Create spec hashing function
3. Modify apply() to check cache first
4. Add cache eviction policy (size limit)
5. Estimated effort: 3-4 hours

---

## üü° MEDIUM: Subgraph Clone in Pipeline (~0.05ms, P4)

### Current Problem
```rust
pub fn py_run_pipeline(
    handle: &PyPipelineHandle,
    subgraph: &PySubgraph,
) -> PyResult<PySubgraph> {
    let subgraph_inner = subgraph.inner.clone();  // ‚ùå Clone HashSets
    let result = pipeline.run(&mut context, subgraph_inner)?;
    PySubgraph::from_core_subgraph(result)
}
```

### Fix Plan: Pipeline Takes Ownership

**Option 1**: Modify Python API to consume graph
```rust
// Python: result = g.apply_consuming(algo)  # g is consumed
pub fn apply_consuming(self, algorithm: &PyAny) -> PyResult<PySubgraph> {
    // Take ownership, no clone needed
    let result = pipeline.run(&mut context, self.inner)?;
    PySubgraph::from_core_subgraph(result)
}
```

**Option 2**: Interior mutability with RefCell
```rust
pub struct Subgraph {
    graph: Rc<RefCell<Graph>>,
    data: RefCell<SubgraphData>,  // Can mutate without &mut
}

// Pipeline can modify in-place
pub fn run(&self, ctx: &mut Context, subgraph: &Subgraph) -> Result<()> {
    subgraph.data.borrow_mut().apply_changes(...);
}
```

**Recommended**: Option 1 - clearer ownership semantics

**Benefits**:
- Eliminates one O(V+E) copy per algorithm
- **Expected: 0.05ms saved**

**Implementation Steps**:
1. Add consuming variant to Python API
2. Update documentation
3. Consider deprecating clone-based version
4. Estimated effort: 2-3 hours

---

## üü¢ LOW PRIORITY: Other Algorithms

### Algorithms to Check

Based on file sizes and patterns, check these for similar issues:

1. **Leiden** (444 lines)
   - Similar to Louvain, likely has same clone issue
   - Check for partition clones in loops

2. **Girvan-Newman** (669 lines)
   - Edge betweenness recalculation
   - Check for repeated neighbor calls

3. **Label Propagation** (299 lines)
   - Iterative label updates
   - Check for per-iteration allocations

4. **PageRank** (already ~3.5ms - good!)
   - No obvious issues

### Pattern to Look For
```rust
for iteration in 0..max_iter {
    for node in nodes {
        let x = expensive_allocation();  // ‚ùå Look for this!
        // or
        let y = expensive_clone();       // ‚ùå And this!
    }
}
```

**Quick wins**: Apply same pattern as Betweenness/Closeness
- Pre-allocate arrays
- Get adjacency snapshot once
- Direct indexing

---

## üìä Priority Matrix

| Bottleneck | Impact | Effort | ROI | Priority |
|------------|--------|--------|-----|----------|
| **Louvain clone** | 4000ms ‚Üí 40ms | Medium | **100x** | üî¥ P0 |
| **Leiden clone** | Unknown, likely high | Medium | ~50x | üî¥ P0 |
| **Lightweight views** | 0.1ms √ó many calls | High | 20% | üü° P1 |
| **Algorithm cache** | 0.05ms √ó many calls | Medium | 15% | üü° P2 |
| **Pipeline ownership** | 0.05ms | Low | 10% | üü¢ P3 |
| **Other algos** | Unknown | Medium | Variable | üü¢ P4 |

---

## üéØ Recommended Execution Order

### Phase 1: Critical Algorithm Fixes (Highest ROI)
1. **Louvain incremental modularity** - 100x speedup potential!
2. **Leiden similar fix** - 50x speedup potential
3. **Quick scan of Girvan-Newman and LPA** - Check for clones

**Estimated time**: 8-12 hours  
**Expected impact**: 50-100x on community detection

### Phase 2: Framework Optimizations
4. **Lightweight subgraph views (P3)** - 20% improvement across all algorithms
5. **Algorithm handle cache (P2)** - 15% improvement on repeated calls

**Estimated time**: 10-12 hours  
**Expected impact**: 15-20% across the board

### Phase 3: Final Polish
6. **Pipeline ownership (P4)** - 10% improvement
7. **Audit remaining algorithms** - Variable gains

**Estimated time**: 6-8 hours  
**Expected impact**: 10-15% on remaining cases

---

## üìà Expected Overall Impact

| Metric | Current | After Phase 1 | After All |
|--------|---------|---------------|-----------|
| **Louvain (500n)** | 4135ms | 40-80ms | 35-70ms |
| **Betweenness** | 76ms | 76ms | 65ms |
| **Closeness** | 46ms | 46ms | 40ms |
| **Fast algos** | 0.9ms | 0.9ms | 0.7ms |

**Total improvement potential**:
- Phase 1: 50-100x on community detection ‚Üê **DO THIS FIRST!**
- Phase 2-3: Additional 20-30% on all algorithms

---

## üî¨ Validation Strategy

For each optimization:

1. **Profile before/after** with `cargo bench` or Python timer
2. **Compare to reference libraries** (NetworkX, igraph, NetworKit)
3. **Verify correctness** - ensure results match unoptimized version
4. **Document changes** - Update algorithm docs with performance notes

---

## üí° Key Takeaways

1. **Louvain is THE bottleneck** - 4+ seconds vs milliseconds for others
2. **Clone in nested loops = disaster** - 100,000x HashMap clones!
3. **Incremental updates >> full recomputation** - Classic optimization
4. **Algorithm optimizations >> framework** - 100x vs 20% impact
5. **Pattern is clear**: Pre-allocate, use deltas, avoid clones in loops

**Next step**: Fix Louvain with incremental modularity - 100x speedup awaits! üöÄ
